{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network Module\n",
        "\n",
        "So far we have looked into the tensors, their properties and basic operations on tensors. These are especially useful to get familiar with if we are building the layers of our network from scratch. We will utilize these in Assignment 3, but moving forward, we will use predefined blocks in the `torch.nn` module of `PyTorch`. We will then put together these blocks to create complex networks. Let's start by importing this module with an alias so that we don't have to type `torch` every time we use it."
      ],
      "metadata": {
        "id": "EtzjclfG5aC_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3LkGtSLy5WM1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Linear Layer**\n",
        "We can use `nn.Linear(H_in, H_out)` to create a a linear layer. This will take a matrix of `(N, *, H_in)` dimensions and output a matrix of `(N, *, H_out)`. The `*` denotes that there could be arbitrary number of dimensions in between. The linear layer performs the operation `Ax+b`, where `A` and `b` are initialized randomly. If we don't want the linear layer to learn the bias parameters, we can initialize our layer with `bias=False`."
      ],
      "metadata": {
        "id": "oU4YpDF05jTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the inputs\n",
        "input = torch.ones(2, 3, 4)\n",
        "# N*H_in -> N*H_out\n",
        "\n",
        "# Make a linear layers transforming N,*,H_in dimensional inputs to\n",
        "# N,*,H_out dimensional outputs\n",
        "linear = nn.Linear(4, 2)\n",
        "linear_output = linear(input)\n",
        "linear_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOxyGYwb5iCU",
        "outputId": "662db597-3fed-4adc-97a4-d616e2b2bde9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.3042, -0.1397],\n",
              "         [-0.3042, -0.1397],\n",
              "         [-0.3042, -0.1397]],\n",
              "\n",
              "        [[-0.3042, -0.1397],\n",
              "         [-0.3042, -0.1397],\n",
              "         [-0.3042, -0.1397]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last dimension of the input needs to match the input of the output layer"
      ],
      "metadata": {
        "id": "TqJ9U1jx6p_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(linear.parameters()) # Ax + b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eik_CIpj6w5k",
        "outputId": "c4719a04-6fc8-49de-fe04-d508d52ae93e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[ 0.1587,  0.0453,  0.0508, -0.3680],\n",
              "         [ 0.2049,  0.2626, -0.3730, -0.0281]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-0.1910, -0.2061], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data of shape [batch_size, feature_dim] # 4\n",
        "# [batch_size, output_dim] # 2\n",
        "\n",
        "# linear layer of shape (feature_dim, output_dim)"
      ],
      "metadata": {
        "id": "H8bjajdb697A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Other Module Layers**\n",
        "There are several other preconfigured layers in the `nn` module. Some commonly used examples are `nn.Conv2d`, `nn.ConvTranspose2d`, `nn.BatchNorm1d`, `nn.BatchNorm2d`, `nn.Upsample` and `nn.MaxPool2d` among many others. We will learn more about these as we progress in the course. For now, the only important thing to remember is that we can treat each of these layers as plug and play components: we will be providing the required dimensions and `PyTorch` will take care of setting them up."
      ],
      "metadata": {
        "id": "aZUhcFHJ-w9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Activation Function Layer**\n",
        "We can also use the `nn` module to apply activations functions to our tensors. Activation functions are used to add non-linearity to our network. Some examples of activations functions are `nn.ReLU()`, `nn.Sigmoid()` and `nn.LeakyReLU()`. Activation functions operate on each element seperately, so the shape of the tensors we get as an output are the same as the ones we pass in."
      ],
      "metadata": {
        "id": "qoTydUwt-zJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJMUUjuU-x40",
        "outputId": "087d0a6d-8c23-4f5f-eb6a-d26b7f37c538"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.3042, -0.1397],\n",
              "         [-0.3042, -0.1397],\n",
              "         [-0.3042, -0.1397]],\n",
              "\n",
              "        [[-0.3042, -0.1397],\n",
              "         [-0.3042, -0.1397],\n",
              "         [-0.3042, -0.1397]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sigmoid = nn.Sigmoid()\n",
        "output = sigmoid(linear_output)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojaEj7FS-32G",
        "outputId": "65e1d205-2233-4db7-c401-45e7a704aa1e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.4245, 0.4651],\n",
              "         [0.4245, 0.4651],\n",
              "         [0.4245, 0.4651]],\n",
              "\n",
              "        [[0.4245, 0.4651],\n",
              "         [0.4245, 0.4651],\n",
              "         [0.4245, 0.4651]]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Putting the Layers Together**\n",
        "So far we have seen that we can create layers and pass the output of one as the input of the next. Instead of creating intermediate tensors and passing them around, we can use `nn.Sequentual`, which does exactly that."
      ],
      "metadata": {
        "id": "bBpNztHU_Aq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block = nn.Sequential(\n",
        "    nn.Linear(4, 2),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "input = torch.ones(2,3,4)\n",
        "output = block(input)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYeLdonx-9zk",
        "outputId": "95ee43a8-539e-4f2d-9932-ac41902be68a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.5690, 0.5417],\n",
              "         [0.5690, 0.5417],\n",
              "         [0.5690, 0.5417]],\n",
              "\n",
              "        [[0.5690, 0.5417],\n",
              "         [0.5690, 0.5417],\n",
              "         [0.5690, 0.5417]]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Modules\n",
        "\n",
        "Instead of using the predefined modules, we can also build our own by extending the `nn.Module` class. For example, we can build a the `nn.Linear` (which also extends `nn.Module`) on our own using the tensor introduced earlier! We can also build new, more complex modules, such as a custom neural network. You will be practicing these in the later assignment.\n",
        "\n",
        "To create a custom module, the first thing we have to do is to extend the `nn.Module`. We can then initialize our parameters in the `__init__` function, starting with a call to the `__init__` function of the super class. All the class attributes we define which are `nn` module objects are treated as parameters, which can be learned during the training. Tensors are not parameters, but they can be turned into parameters if they are wrapped in `nn.Parameter` class.\n",
        "\n",
        "All classes extending `nn.Module` are also expected to implement a `forward(x)` function, where `x` is a tensor. This is the function that is called when a parameter is passed to our module, such as in `model(x)`."
      ],
      "metadata": {
        "id": "UMzydqbs_Pvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultilayerPreceptron(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    # Call to the __init__ function of the super class\n",
        "    super(MultilayerPreceptron, self).__init__()\n",
        "\n",
        "    # Bookkeeping: Saving the initialization parameters\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # Defining our model\n",
        "    # There isn't anything specific about the naming of `self.model`\n",
        "    # It could be something arbitrary\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(self.input_size, self.hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.hidden_size, self.input_size),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.model(x)\n",
        "    return output"
      ],
      "metadata": {
        "id": "e6nnFGhp_OFT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a sample input\n",
        "input = torch.randn(2, 5)\n",
        "\n",
        "# Create our model\n",
        "model = MultilayerPreceptron(5, 3)\n",
        "\n",
        "# Pass out input through our model\n",
        "model(input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwF52ZzNAWas",
        "outputId": "5fd836b2-de0f-43ce-8718-291aaea9a288"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.6355, 0.5599, 0.7260, 0.7225, 0.6926],\n",
              "        [0.6583, 0.5405, 0.7435, 0.7074, 0.6545]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(model.named_parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80lI3xnSAiRX",
        "outputId": "fc230d96-f852-4df2-f145-53335b783d4f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('model.0.weight',\n",
              "  Parameter containing:\n",
              "  tensor([[-0.2593,  0.3496,  0.2565, -0.0212,  0.1479],\n",
              "          [-0.3470,  0.2189,  0.2929, -0.3131,  0.2926],\n",
              "          [-0.1784, -0.0630, -0.1378, -0.1042, -0.2313]], requires_grad=True)),\n",
              " ('model.0.bias',\n",
              "  Parameter containing:\n",
              "  tensor([ 0.1755,  0.4140, -0.3019], requires_grad=True)),\n",
              " ('model.2.weight',\n",
              "  Parameter containing:\n",
              "  tensor([[ 0.3475, -0.1371,  0.2973],\n",
              "          [-0.1524,  0.2370, -0.2218],\n",
              "          [ 0.5438,  0.1234, -0.5754],\n",
              "          [-0.0465,  0.3260,  0.5406],\n",
              "          [-0.2919,  0.5702,  0.0846]], requires_grad=True)),\n",
              " ('model.2.bias',\n",
              "  Parameter containing:\n",
              "  tensor([0.4732, 0.0418, 0.3950, 0.5584, 0.2759], requires_grad=True))]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimization\n",
        "We have showed how gradients are calculated with the `backward()` function. Having the gradients isn't enought for our models to learn. We also need to know how to update the parameters of our models. This is where the optomozers comes in. `torch.optim` module contains several optimizers that we can use. Some popular examples are `optim.SGD` and `optim.Adam`. When initializing optimizers, we pass our model parameters, which can be accessed with `model.parameters()`, telling the optimizers which values it will be optimizing. Optimizers also has a learning rate (`lr`) parameter, which determines how big of an update will be made in every step. Different optimizers have different hyperparameters as well."
      ],
      "metadata": {
        "id": "-k65-fCXBdNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "0JRsgK4YA4pl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After we have our optimization function, we can define a `loss` that we want to optimize for. We can either define the loss ourselves, or use one of the predefined loss function in `PyTorch`, such as `nn.BCELoss()`. Let's put everything together now! We will start by creating some dummy data."
      ],
      "metadata": {
        "id": "MLNQJSWEBieb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the y data\n",
        "y = torch.ones(10, 5)\n",
        "\n",
        "# Add some noise to our goal y to generate our x\n",
        "# We want our model to predict our original data, albeit the noise\n",
        "x = y + torch.randn_like(y)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqkbPMSnBmiu",
        "outputId": "f48f78c0-7a4e-49fd-f5f5-b9daee184474"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.9643,  0.8302, -0.3312,  0.5233,  0.8402],\n",
              "        [ 0.7868,  1.3532,  0.4634,  2.2927,  1.9026],\n",
              "        [ 1.1020,  1.7708,  0.9847,  0.4543,  0.5615],\n",
              "        [-0.0502,  1.3032,  1.2882,  1.5763, -0.1869],\n",
              "        [-0.6478, -0.4904, -0.1440,  0.0830,  0.4464],\n",
              "        [ 1.0662,  1.4892,  1.5628,  1.8956,  0.7481],\n",
              "        [ 0.2444,  1.1906,  1.5316,  0.3316,  0.9216],\n",
              "        [ 0.8383,  1.1177,  0.2687, -1.2358, -0.0720],\n",
              "        [ 0.3813,  0.7183,  0.9800,  1.9383,  1.1709],\n",
              "        [ 0.8400, -0.4514,  0.4877,  2.1526,  1.5566]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can define our model, optimizer and the loss function."
      ],
      "metadata": {
        "id": "gB9GvFmvB-Yn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = MultilayerPreceptron(5, 3)\n",
        "\n",
        "# Define the optimizer\n",
        "adam = optim.Adam(model.parameters(), lr=1e-1)\n",
        "\n",
        "# Define loss using a predefined loss function\n",
        "loss_function = nn.BCELoss()\n",
        "\n",
        "# Calculate how our model is doing\n",
        "y_pred = model(x)\n",
        "loss_function(y_pred, y).item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pi5sQwBIB3a7",
        "outputId": "dbe89478-ccd1-42f4-a0ba-7ea9ca9b0e82"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6285327672958374"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if we can have our model achieve a smaller loss. Now that we have everything we need, we can setup our training loop."
      ],
      "metadata": {
        "id": "U8CcWnDoCeJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the number of epoch, which determines the number of training iterations\n",
        "n_epoch = 100\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "  # Set the gradients to 0\n",
        "  adam.zero_grad()\n",
        "\n",
        "  # Get the model predictions\n",
        "  y_pred = model(x)\n",
        "\n",
        "  # Get the loss\n",
        "  loss = loss_function(y_pred, y)\n",
        "\n",
        "  # Print stats\n",
        "  print(f\"Epoch {epoch}: training loss: {loss}\")\n",
        "\n",
        "  # Compute the gradients\n",
        "  loss.backward()\n",
        "\n",
        "  # Take a step to optimize the weights\n",
        "  adam.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4Ojk-taCPcG",
        "outputId": "c8fb5308-7505-4835-9aab-af19501f2bb4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: training loss: 0.046968575567007065\n",
            "Epoch 1: training loss: 0.040236763656139374\n",
            "Epoch 2: training loss: 0.035454392433166504\n",
            "Epoch 3: training loss: 0.031939342617988586\n",
            "Epoch 4: training loss: 0.029276227578520775\n",
            "Epoch 5: training loss: 0.027201585471630096\n",
            "Epoch 6: training loss: 0.025465665385127068\n",
            "Epoch 7: training loss: 0.023145359009504318\n",
            "Epoch 8: training loss: 0.020726410672068596\n",
            "Epoch 9: training loss: 0.018294090405106544\n",
            "Epoch 10: training loss: 0.015935909003019333\n",
            "Epoch 11: training loss: 0.013725428842008114\n",
            "Epoch 12: training loss: 0.011714156717061996\n",
            "Epoch 13: training loss: 0.009930022060871124\n",
            "Epoch 14: training loss: 0.008380299434065819\n",
            "Epoch 15: training loss: 0.007056694012135267\n",
            "Epoch 16: training loss: 0.0059408340603113174\n",
            "Epoch 17: training loss: 0.005009099375456572\n",
            "Epoch 18: training loss: 0.004236235283315182\n",
            "Epoch 19: training loss: 0.0035977547522634268\n",
            "Epoch 20: training loss: 0.003071337006986141\n",
            "Epoch 21: training loss: 0.002637388650327921\n",
            "Epoch 22: training loss: 0.0022792546078562737\n",
            "Epoch 23: training loss: 0.0019830160308629274\n",
            "Epoch 24: training loss: 0.0017372213769704103\n",
            "Epoch 25: training loss: 0.0015325075946748257\n",
            "Epoch 26: training loss: 0.0013612998882308602\n",
            "Epoch 27: training loss: 0.0012174607254564762\n",
            "Epoch 28: training loss: 0.0010960575891658664\n",
            "Epoch 29: training loss: 0.0009930824162438512\n",
            "Epoch 30: training loss: 0.0009053149842657149\n",
            "Epoch 31: training loss: 0.0008301634807139635\n",
            "Epoch 32: training loss: 0.0007654873770661652\n",
            "Epoch 33: training loss: 0.0007095632026903331\n",
            "Epoch 34: training loss: 0.0006609923439100385\n",
            "Epoch 35: training loss: 0.0006186128011904657\n",
            "Epoch 36: training loss: 0.0005814807955175638\n",
            "Epoch 37: training loss: 0.0005488012102432549\n",
            "Epoch 38: training loss: 0.0005199307342991233\n",
            "Epoch 39: training loss: 0.00049431441584602\n",
            "Epoch 40: training loss: 0.0004715078976005316\n",
            "Epoch 41: training loss: 0.00045111612416803837\n",
            "Epoch 42: training loss: 0.00043282395927235484\n",
            "Epoch 43: training loss: 0.0004163572157267481\n",
            "Epoch 44: training loss: 0.0004014850710518658\n",
            "Epoch 45: training loss: 0.00038800216862000525\n",
            "Epoch 46: training loss: 0.00037573801819235086\n",
            "Epoch 47: training loss: 0.0003645534161478281\n",
            "Epoch 48: training loss: 0.00035431861761026084\n",
            "Epoch 49: training loss: 0.0003449246578384191\n",
            "Epoch 50: training loss: 0.00033627424272708595\n",
            "Epoch 51: training loss: 0.0003282907709944993\n",
            "Epoch 52: training loss: 0.00032089624437503517\n",
            "Epoch 53: training loss: 0.0003140284097753465\n",
            "Epoch 54: training loss: 0.0003076261782553047\n",
            "Epoch 55: training loss: 0.0003016571281477809\n",
            "Epoch 56: training loss: 0.0002960757410619408\n",
            "Epoch 57: training loss: 0.00029082465334795415\n",
            "Epoch 58: training loss: 0.00028588101849891245\n",
            "Epoch 59: training loss: 0.00028121855575591326\n",
            "Epoch 60: training loss: 0.00027680848143063486\n",
            "Epoch 61: training loss: 0.0002726388047449291\n",
            "Epoch 62: training loss: 0.0002686641237232834\n",
            "Epoch 63: training loss: 0.00026488679577596486\n",
            "Epoch 64: training loss: 0.0002612709649838507\n",
            "Epoch 65: training loss: 0.0002578165731392801\n",
            "Epoch 66: training loss: 0.00025451520923525095\n",
            "Epoch 67: training loss: 0.0002513406507205218\n",
            "Epoch 68: training loss: 0.00024828329333104193\n",
            "Epoch 69: training loss: 0.0002453407214488834\n",
            "Epoch 70: training loss: 0.00024250574642792344\n",
            "Epoch 71: training loss: 0.00023976167722139508\n",
            "Epoch 72: training loss: 0.00023709893866907805\n",
            "Epoch 73: training loss: 0.00023451865126844496\n",
            "Epoch 74: training loss: 0.00023201967997010797\n",
            "Epoch 75: training loss: 0.0002295853046234697\n",
            "Epoch 76: training loss: 0.0002272166748298332\n",
            "Epoch 77: training loss: 0.00022491140407510102\n",
            "Epoch 78: training loss: 0.0002226599317509681\n",
            "Epoch 79: training loss: 0.00022045630612410605\n",
            "Epoch 80: training loss: 0.00021830768673680723\n",
            "Epoch 81: training loss: 0.00021620685583911836\n",
            "Epoch 82: training loss: 0.00021415148512460291\n",
            "Epoch 83: training loss: 0.00021213672880548984\n",
            "Epoch 84: training loss: 0.0002101650316035375\n",
            "Epoch 85: training loss: 0.00020821724319830537\n",
            "Epoch 86: training loss: 0.00020631728693842888\n",
            "Epoch 87: training loss: 0.00020444839901756495\n",
            "Epoch 88: training loss: 0.000202614173758775\n",
            "Epoch 89: training loss: 0.00020080030662938952\n",
            "Epoch 90: training loss: 0.00019902583153452724\n",
            "Epoch 91: training loss: 0.0001972705649677664\n",
            "Epoch 92: training loss: 0.00019554991740733385\n",
            "Epoch 93: training loss: 0.00019385319319553673\n",
            "Epoch 94: training loss: 0.00019218752277083695\n",
            "Epoch 95: training loss: 0.00019053502182941884\n",
            "Epoch 96: training loss: 0.0001889135892270133\n",
            "Epoch 97: training loss: 0.00018731009913608432\n",
            "Epoch 98: training loss: 0.00018573526176624\n",
            "Epoch 99: training loss: 0.000184177202754654\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3ofwQrGDKhZ",
        "outputId": "ee510bc2-de59-4d31-d8e3-62425d7654d5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[-0.0182,  1.6984,  1.4604,  1.5639,  1.4134],\n",
              "         [ 0.1076, -0.2563,  0.1508, -0.1516, -0.2936],\n",
              "         [-0.4619, -0.8205, -1.1260, -0.0103, -0.0219]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([ 2.3666, -0.2720, -0.9283], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([[ 2.0084,  0.0029,  1.1707],\n",
              "         [ 2.4318, -0.4874,  0.3727],\n",
              "         [ 2.3818, -0.1265,  0.4126],\n",
              "         [ 2.1378, -0.5300,  0.2120],\n",
              "         [ 2.3393,  0.0768,  0.3778]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([2.2316, 1.7545, 2.1739, 1.4753, 2.1327], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model(x)\n",
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzcIja5yDOwg",
        "outputId": "fca0f653-9987-4fff-9f6d-95b43332db85"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
              "        [0.9984, 0.9989, 0.9992, 0.9974, 0.9991],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
              "        [0.9994, 0.9997, 0.9998, 0.9991, 0.9997],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create test data and check how our model performs on it\n",
        "x2 = y + torch.randn_like(y)\n",
        "y_pred = model(x2)\n",
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxTkdWeGDY2J",
        "outputId": "1f4d8b5d-8037-440f-b8f2-dc741ebbfb6b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
              "        [0.9999, 0.9999, 1.0000, 0.9998, 1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! Looks like our model almost perfectly learned to filter out the noise from the `x` that we passed in!"
      ],
      "metadata": {
        "id": "Dnj21o4qD5Wz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "if39HuTOECqK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}